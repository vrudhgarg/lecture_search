{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing notebook: DSCI_554/appendix_blocking_and_stratification.ipynb\n",
      "Processing notebook: DSCI_554/lecture1_multiple_comparisons.ipynb\n",
      "Processing notebook: DSCI_554/lecture7_obs_sampling_schemes.ipynb\n",
      "Processing notebook: DSCI_554/appendix-reg-mindmap.ipynb\n",
      "Processing notebook: DSCI_554/appendix-causality-exp-cheatsheet.ipynb\n",
      "Processing notebook: DSCI_554/lecture3_randomization_and_blocking.ipynb\n",
      "Processing notebook: DSCI_554/appendix-reg-cheatsheet.ipynb\n",
      "Processing notebook: DSCI_554/appendix-greek-alphabet.ipynb\n",
      "Processing notebook: DSCI_554/lecture5_more_power_early_stopping.ipynb\n",
      "Processing notebook: DSCI_554/lecture8_match_constrasts_wrapup.ipynb\n",
      "Processing notebook: DSCI_554/lecture2_simpson_confounding.ipynb\n",
      "Processing notebook: DSCI_554/appendix-dist-cheatsheet.ipynb\n",
      "Processing notebook: DSCI_554/lecture4_more_blocking_and_power.ipynb\n",
      "Processing notebook: DSCI_554/appendix-prob-cheatsheet.ipynb\n",
      "Processing notebook: DSCI_554/lecture6_obs_stratifying_modelling.ipynb\n",
      "Processing PDF: DSCI_541/lec2-disinformation.pdf\n",
      "Processing PDF: DSCI_541/lec3-privacy.pdf\n",
      "Processing PDF: DSCI_541/lec8-balancing-fairness.pdf\n",
      "Processing PDF: DSCI_541/lec1-intro.pdf\n",
      "Processing PDF: DSCI_541/lec7-quantifying-fairness.pdf\n",
      "Processing PDF: DSCI_541/lec4-anonymization.pdf\n",
      "Processing PDF: DSCI_541/lec5-regulation-and-bias.pdf\n",
      "Processing notebook: DSCI_541/lec8-fairness-matrix.ipynb\n",
      "Processing PDF: DSCI_541/lec6-harmful-bias.pdf\n",
      "Processing notebook: DSCI_525/lecture6.ipynb\n",
      "Processing notebook: DSCI_525/Tutorial3_Flask.ipynb\n",
      "Processing notebook: DSCI_525/lecture3-resource_view+s3.ipynb\n",
      "Processing notebook: DSCI_525/lecture0.ipynb\n",
      "Processing notebook: DSCI_525/lecture2.ipynb\n",
      "Processing notebook: DSCI_525/lecture7.ipynb\n",
      "Processing notebook: DSCI_525/lecture1.ipynb\n",
      "Processing notebook: DSCI_525/lecture4-architectures+keys.ipynb\n",
      "Processing notebook: DSCI_525/PySpark_ML_Tutorial_2025-EMRNotebook.ipynb\n",
      "Processing notebook: DSCI_575/08_llms-applications.ipynb\n",
      "Processing notebook: DSCI_575/03_HMMs-intro.ipynb\n",
      "Processing notebook: DSCI_575/06_intro-to-transformers.ipynb\n",
      "Processing notebook: DSCI_575/01_Markov-models.ipynb\n",
      "Processing notebook: DSCI_575/05_intro-to-RNNs.ipynb\n",
      "Processing notebook: DSCI_575/04_More-HMMs.ipynb\n",
      "Processing notebook: DSCI_575/AppendixA-BaumWelch.ipynb\n",
      "Processing notebook: DSCI_575/00_course-information.ipynb\n",
      "Processing notebook: DSCI_575/02_LMs-text-preprocessing.ipynb\n",
      "Processing notebook: DSCI_575/07_more-transformers.ipynb\n",
      "\n",
      "‚úÖ Total chunks created: 1713\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import NotebookLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_chunk_ipynb(ipynb_path):\n",
    "    loader = NotebookLoader(\n",
    "        ipynb_path,\n",
    "        include_outputs=True,\n",
    "        max_output_length=500\n",
    "    )\n",
    "    documents = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "def load_and_chunk_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# üîÅ Recursively walk through ./lectures and process .ipynb and .pdf\n",
    "ipynb_dir = './lectures/'\n",
    "all_chunks = []\n",
    "\n",
    "for root, dirs, files in os.walk(ipynb_dir):\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        rel_path = os.path.relpath(file_path, start=ipynb_dir)\n",
    "\n",
    "        if filename.endswith(\".ipynb\"):\n",
    "            print(f\"Processing notebook: {rel_path}\")\n",
    "            chunks = load_and_chunk_ipynb(file_path)\n",
    "\n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            print(f\"Processing PDF: {rel_path}\")\n",
    "            chunks = load_and_chunk_pdf(file_path)\n",
    "\n",
    "        else:\n",
    "            continue  # Skip non-supported files\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata[\"source\"] = rel_path\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\n‚úÖ Total chunks created: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/m638s3x919b6zv04xt14_j8w0000gn/T/ipykernel_13066/1460721389.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
      "/Users/dhruvgarg/miniforge3/envs/test_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "# You'll need to choose one of these for your LLM\n",
    "# from langchain_community.llms import HuggingFacePipeline # For local LLMs\n",
    "# from langchain_openai import ChatOpenAI # For OpenAI API\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI # For Google Gemini API\n",
    "\n",
    "\n",
    "# --- 1. Load the Embeddings Model (MUST be the same as used for indexing) ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Save chunks to Chroma DB\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"example_collection\",\n",
    "    persist_directory=\"./chroma_langchain_db\"\n",
    ")\n",
    "\n",
    "print(\"Vector store saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define the Retriever ---\n",
    "# This converts your vector store into a retriever\n",
    "# search_kwargs={\"k\": 3} means it will retrieve the top 3 most relevant chunks\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI GPT-3.5-turbo.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "# Set your OpenAI API key as an environment variable (e.g., in your .bashrc or .zshrc)\n",
    "# export OPENAI_API_KEY=\"your_api_key_here\"\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "    print(\"Using OpenAI GPT-3.5-turbo.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY environment variable not set. Cannot use OpenAI LLM.\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asking: What is confounding variables?\n",
      "\n",
      "--- Generated Answer ---\n",
      "Confounding variables are third factors associated with the exposure that independently affect the risk of developing the disease, and they can affect the relationship between the exposure and the outcome in a regression framework.\n",
      "\n",
      "--- Sources Used ---\n",
      "  Source: appendix-causality-exp-cheatsheet.ipynb\n",
      "  Source: appendix-causality-exp-cheatsheet.ipynb\n",
      "  Source: lecture7_obs_sampling_schemes.ipynb\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Set up the RAG Chain ---\n",
    "if llm:\n",
    "    # Define a custom prompt template for the LLM\n",
    "    # This guides the LLM on how to use the context.\n",
    "    template = \"\"\"Use the following pieces of context from my lecture notes to answer the question at the end.\n",
    "    If you don't know the answer based on the provided context, just say that you don't know.\n",
    "    Do not make up an answer or use outside knowledge.\n",
    "    Be concise and clear in your response.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Create the RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\", # 'stuff' means it will combine all retrieved docs into one prompt.\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True, # This will show you which chunks were used\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT} # Pass your custom prompt\n",
    "    )\n",
    "\n",
    "    # --- 6. Ask a Question! ---\n",
    "    query = \"What is confounding variables?\" # Example query\n",
    "    # Replace with a question relevant to your lecture content\n",
    "\n",
    "    print(f\"\\nAsking: {query}\")\n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "    print(\"\\n--- Generated Answer ---\")\n",
    "    print(response[\"result\"])\n",
    "\n",
    "    print(\"\\n--- Sources Used ---\")\n",
    "    if response[\"source_documents\"]:\n",
    "        for doc in response[\"source_documents\"]:\n",
    "            # LangChain's Document objects have .page_content and .metadata\n",
    "            print(f\"  Source: {doc.metadata.get('source', 'Unknown File')}\")\n",
    "            # print(f\"  Content Snippet: {doc.page_content[:200]}...\") # Uncomment to see a snippet of the chunk\n",
    "    else:\n",
    "        print(\"No source documents were retrieved for this query.\")\n",
    "else:\n",
    "    print(\"\\nRAG chain cannot be run because no LLM was successfully initialized.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
